{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "-*- coding: utf-8 -*-<br>\n", "Evaluation for the prior item/name model<br>\n", "@inproceedings{majumder2019emnlp,<br>\n", "  title={Generating Personalized Recipes from Historical User Preferences},<br>\n", "  author={Majumder, Bodhisattwa Prasad* and Li, Shuyang* and Ni, Jianmo and McAuley, Julian},<br>\n", "  booktitle={EMNLP},<br>\n", "  year={2019}<br>\n", "}<br>\n", "Copyright Shuyang Li & Bodhisattwa Majumder<br>\n", "License: GNU GPLv3<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import torch\n", "import numpy as np\n", "import pickle\n", "import torch.utils.data as data\n", "import torch.nn as nn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from functools import partial\n", "from tqdm import tqdm\n", "from itertools import chain\n", "from datetime import datetime\n", "from recipe_gen.language import START_INDEX, PAD_INDEX, TECHNIQUES_LIST, pretty_decode, decode_ids, END_INDEX\n", "from recipe_gen.pipeline.train import train_model\n", "from recipe_gen.pipeline.batch import get_batch_information_general, get_user_prior_item_mask\n", "from recipe_gen.pipeline.eval import top_k_logits, sample_next_token"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Filters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["MAX_NAME = 15\n", "MAX_INGR = 5\n", "MAX_INGR_TOK = 20\n", "MAX_STEP_TOK = 256"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eval_model(device, model, sampler, loss_compute, logit_modifier_fxn, token_sampler,\n", "               print_every, max_len,\n", "               user_items_df, top_k, pad_item_ix,\n", "               max_name_len=15, ingr_map=None, \n", "               base_save_dir='', pad_ingr=None, ppx_only=False, **tensor_kwargs):\n", "    \"\"\"\n", "    Run a single epoch\n", "    Arguments:\n", "        device {torch.device} -- Torch device on which to store/process data\n", "        model {nn.Module} -- Model to be trained/run\n", "        sampler {BatchSampler} -- Data sampler\n", "        loss_compute {func} -- Function to compute loss for each batch\n", "        logit_modifier_fxn {func} -- Function to modify a logit distr. and return a prob. distro\n", "        token_sampler {str} -- \"greedy\" or \"multinomial\"\n", "        print_every {int} -- Log loss every k iterations\n", "        max_len {int} -- Maximum length / number of steps to unroll and predict\n", "    Keyword Arguments:\n", "        max_name_len {int} -- Maximum # timesteps to unroll to predict name (default: {15})\n", "        ingr_map {dict} -- Map of ingredient ID -> ingredient raw name.\n", "        pad_ingr {int} -- Index of pad item (default: {None})\n", "        base_save_dir {str} -- Base folder in which to save experiments\n", "        ppx_only {bool} -- Only calculate test perplexity (default: {False})\n", "        **tensor_kwargs {torch.Tensor} -- Assorted tensors for fun and profit\n", "    Returns:\n", "        float -- Average loss across the epoch\n", "    \"\"\"\n", "    start = datetime.now()\n", "    results_dicts = []\n\n", "    # Extract into tuples and list\n", "    tensor_names, base_tensors = zip(*tensor_kwargs.items())\n\n", "    # Iterate through batches in the epoch\n", "    model.eval()\n", "    with torch.no_grad():\n", "        total_tokens = 0\n", "        total_name_tokens = 0\n", "        total_loss = 0.0\n", "        total_name_loss = 0.0\n", "        print_tokens = 0\n", "        for i, batch in enumerate(tqdm(sampler.epoch_batches(), total=sampler.n_batches), 1):\n", "            batch_users, items = [t.to(device) for t in batch]\n\n", "            # Get prior items and their masks\n", "            tuple_user_item_masks = [get_user_prior_item_mask(\n", "                        user_ix=uix.item(), \n", "                        item_ix=iix.item(), \n", "                        user_items_df=user_items_df, \n", "                        top_k=top_k, \n", "                        pad_item_ix=pad_item_ix)\n", "            for uix, iix in zip(batch_users, items)]\n", "            user_items, user_item_masks = [\n", "                torch.LongTensor(t).to(device) for t in zip(*tuple_user_item_masks)\n", "            ]\n", "            user_prior_names = torch.stack([\n", "                torch.index_select(tensor_kwargs['name_tensor'], 0, u_i) for u_i in user_items\n", "            ], dim=0).to(device)\n\n", "            # Fill out batch information\n", "            batch_map = dict(zip(\n", "                tensor_names,\n", "                get_batch_information_general(items, *base_tensors)\n", "            ))\n", "            use_ingr_embedding = batch_map['ingr_tensor'].size(-1) != MAX_INGR * MAX_INGR_TOK\n\n", "            # Logistics\n", "            this_batch_size = batch_map['steps_tensor'].size(0)\n", "            this_batch_num_tokens = (batch_map['steps_tensor'] != PAD_INDEX).data.sum().item()\n", "            this_batch_num_name_tokens = 0\n", "            this_batch_num_name_tokens = (batch_map['name_tensor'] != PAD_INDEX).data.sum().item()\n", "            name_targets = batch_map['name_tensor'][:, :-1]\n", "            '''\n", "            Teacher forcing - evaluate\n", "            '''\n", "            # Comparing out(token[t-1]) to token[t]\n", "            (log_probs, _), (name_log_probs, _) = model.forward(\n", "                device=device, inputs=(\n", "                    batch_map['calorie_level_tensor'],\n", "                    batch_map['name_tensor'],\n", "                    batch_map['ingr_tensor']\n", "                ),\n", "                ingr_masks=batch_map['ingr_mask_tensor'],\n", "                user_item_names=user_prior_names,\n", "                user_items=user_items, user_item_masks=user_item_masks,\n", "                targets=batch_map['steps_tensor'][:, :-1], max_len=max_len-1,\n", "                start_token=START_INDEX, teacher_forcing=True,\n", "                name_targets=name_targets,\n", "                max_name_len=max_name_len-1,\n", "                visualize=False\n", "            )\n", "            loss, name_loss = loss_compute(\n", "                log_probs, batch_map['steps_tensor'][:, 1:],\n", "                name_outputs=name_log_probs,\n", "                name_targets=name_targets,\n", "                norm=this_batch_size,\n", "                model=model,\n", "                clip=None\n", "            )\n", "            total_loss += loss\n", "            total_name_loss += name_loss\n\n", "            # Logging\n", "            total_tokens += this_batch_num_tokens\n", "            total_name_tokens += this_batch_num_name_tokens\n", "            print_tokens += this_batch_num_tokens\n", "            del log_probs, name_log_probs\n\n", "            # Short-circuit if we only want to calculate test perplexity\n", "            if ppx_only:\n", "                if i % print_every == 0:\n", "                    elapsed = datetime.now() - start\n", "                    print(\"Epoch Step: {} LM Loss: {:.5f}; Name Loss: {:.5f}; Tok/s: {:.3f}\".format(\n", "                        i, loss / this_batch_size, name_loss / this_batch_size,\n", "                        print_tokens / elapsed.seconds\n", "                    ))\n", "                    start = datetime.now()\n", "                    print_tokens = 0\n", "                continue\n", "            '''\n", "            Non-teacher-forcing - Generate!\n", "            '''\n", "            # Generates probabilities\n", "            (log_probs, output_tokens, ingr_attns, prior_item_attns), \\\n", "            (name_log_probs, name_output_tokens) = model.forward(\n", "                device=device, inputs=(\n", "                    batch_map['calorie_level_tensor'],\n", "                    batch_map['name_tensor'],\n", "                    batch_map['ingr_tensor']\n", "                ),\n", "                ingr_masks=batch_map['ingr_mask_tensor'],\n", "                user_item_names=user_prior_names,\n", "                user_items=user_items, user_item_masks=user_item_masks,\n", "                targets=batch_map['steps_tensor'][:, :-1], max_len=max_len-1,\n", "                start_token=START_INDEX, teacher_forcing=False,\n", "                logit_modifier_fxn=logit_modifier_fxn, token_sampler=token_sampler,\n", "                visualize=True, max_name_len=max_name_len-1, name_targets=name_targets,\n", "            )\n", "            del log_probs, name_log_probs\n\n", "            # Generated recipe\n", "            calorie_levels, technique_strs, ingredient_strs, gold_strs, generated_strs, \\\n", "                prior_items, recipe_reprs = get_batch_generated_recipes(\n", "                    batch_users=batch_users, batch_generated=output_tokens,\n", "                    max_ingr=MAX_INGR, max_ingr_tok=MAX_INGR_TOK,\n", "                    names_generated=name_output_tokens, ingr_map=ingr_map,\n", "                    user_items_df=user_items_df, **batch_map\n", "                )\n", "            for ix in range(len(generated_strs)):\n", "                # Create save location: test_i<item>_u<user>\n", "                ii = items[ix].data.item()\n", "                uu = batch_users[ix].data.item()\n", "                sample_id = 'test_i{}_u{}'.format(ii, uu)\n", "                trial_save_dir = os.path.join(base_save_dir, sample_id)\n", "                if not os.path.exists(trial_save_dir):\n", "                    os.mkdir(trial_save_dir)\n", "                # Output tokens for heatmap axes\n", "                out_indices = output_tokens[ix].detach().cpu().numpy().tolist()\n", "                out_tokens = decode_ids(out_indices)\n", "                trunc_indices = out_indices[:out_indices.index(END_INDEX)] \\\n", "                    if END_INDEX in out_indices else out_indices\n", "                output_len = len(trunc_indices)\n", "                output_techniques = [t for t in TECHNIQUES_LIST if t in generated_strs[ix]]\n", "                results_dicts.append({\n", "                    'u': uu,\n", "                    'i': ii,\n", "                    'generated': generated_strs[ix],\n", "                    'n_tokens': output_len,\n", "                    'generated_techniques': output_techniques,\n", "                    'n_techniques': len(output_techniques)\n", "                })\n", "                # Save output\n", "                with open(os.path.join(trial_save_dir, 'output.txt'), 'w+', encoding='utf-8') as wf:\n", "                    wf.write(recipe_reprs[ix])\n", "                # Ingredient Attention\n", "                ingr_attentions = np.matrix([\n", "                    a.squeeze().detach().cpu().numpy().tolist() for a in ingr_attns[ix]\n", "                ]).T\n", "                ingr_attn_df = pd.DataFrame(\n", "                    ingr_attentions[:len(ingredient_strs[ix])],\n", "                    index=ingredient_strs[ix], columns=out_tokens\n", "                )\n", "                ingr_attn_df = ingr_attn_df[ingr_attn_df.index != '']\n", "                ingr_attn_df.to_pickle(\n", "                    os.path.join(trial_save_dir, 'ingredient_attention.pkl')\n", "                )\n", "                # Prior Item Attention\n", "                prior_item_attentions = np.matrix([\n", "                    a.squeeze().detach().cpu().numpy().tolist() for a in prior_item_attns[ix]\n", "                ]).T\n", "                user_item_ids = user_items[ix].detach().cpu().numpy().tolist()\n", "                user_item_names = [\n", "                    pretty_decode(df_r.loc[iid, 'name_tokens']) if iid in df_r.index else ''\n", "                    for iid in user_item_ids\n", "                ]\n", "                prior_item_attn_df = pd.DataFrame(\n", "                    prior_item_attentions,\n", "                    index=user_item_names, columns=out_tokens\n", "                )\n", "                prior_item_attn_df = prior_item_attn_df[prior_item_attn_df.index != '']\n", "                prior_item_attn_df.to_pickle(\n", "                    os.path.join(trial_save_dir, 'prior_item_attention.pkl')\n", "                )\n", "            if i % print_every == 0:\n", "                elapsed = datetime.now() - start\n", "                print(\"Epoch Step: {} LM Loss: {:.5f}; Name Loss: {:.5f}; Tok/s: {:.3f}\".format(\n", "                    i, loss / this_batch_size, name_loss / this_batch_size,\n", "                    print_tokens / elapsed.seconds\n", "                ))\n", "                print('SAMPLE DECODED RECIPE:\\n\\n{}\\n\\n'.format(recipe_reprs[0]))\n", "                start = datetime.now()\n", "                print_tokens = 0\n\n", "        # Reshuffle the sampler\n", "        sampler.renew_indices()\n", "        if total_name_tokens > 0:\n", "            print('\\nName Perplexity: {}'.format(\n", "                np.exp(total_name_loss / float(total_name_tokens))\n", "            ))\n\n", "        # Store perplexity\n", "        ppx = np.exp(total_loss / float(total_tokens))\n", "        with open(os.path.join(base_save_dir, 'ppx.pkl'), 'wb') as wf:\n", "            pickle.dump(ppx, wf)\n", "        print('PERPLEXITY: {:.5f}'.format(\n", "            ppx\n", "        ))\n", "        if not ppx_only:\n", "            # Store recipe information -- generated string, # tokens (length), tech, # tech\n", "            gen_df = pd.DataFrame(results_dicts)[[\n", "                'u', 'i', 'generated', 'n_tokens', 'generated_techniques', 'n_techniques'\n", "            ]]\n", "            df_loc = os.path.join(base_save_dir, 'generated_df.pkl')\n", "            gen_df.to_pickle(df_loc)\n", "            print('Saved generation DF to {}'.format(\n", "                df_loc\n", "            ))\n", "            print(gen_df.head(3))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "==== RUN CLUSTER ALL (PRIOR NAME ATTN)<br>\n", "python -m recipe_gen.models.user_item_pref.test --data-dir <DATA FOLDER> --model-path <MODEL PATH> --vocab-emb-size 300 --calorie-emb-size 5 --top-k 20 --nhid 256 --nlayers 2 --save-dir <OUTPUT FOLDER> --overwrite --batch-size 48 --ingr-emb-size 10  --ingr-gru --ingr-emb<br>\n", "==== RUN CLUSTER ALL (PRIOR ITEM ATTN)<br>\n", "python -m recipe_gen.models.user_item_pref.test --data-dir <DATA FOLDER> --model-path <MODEL PATH> --vocab-emb-size 300 --calorie-emb-size 5 --top-k 20 --nhid 256 --nlayers 2 --save-dir <OUTPUT FOLDER> --overwrite --batch-size 48 --ingr-emb-size 10  --ingr-gru --ingr-emb --item-emb --item-emb-size 50<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    import os\n", "    import torch\n", "    import argparse\n", "    import torch.nn as nn\n", "    import numpy as np\n", "    import pandas as pd\n", "    import pickle\n", "    from datetime import datetime\n", "    from functools import partial\n", "    from itertools import chain\n", "    from recipe_gen.utils import get_device, count_parameters\n", "    from recipe_gen.language import N_TECHNIQUES, VOCAB_SIZE, START_INDEX, TECHNIQUES_LIST, decode_ids, PAD_INDEX\n", "    from recipe_gen.pipeline import DataFrameDataset, BatchSampler\n", "    from recipe_gen.pipeline.train import SimpleLossCompute\n", "    from recipe_gen.pipeline.batch import load_full_data, pad_recipe_info, load_recipe_tensors\n", "    from . import create_model\n", "    from recipe_gen.pipeline.visualization import get_tag_ix_batches_from_data, get_batch_generated_recipes\n", "    from recipe_gen.pipeline.eval import top_k_logits, sample_next_token, top_p_logits\n", "    start = datetime.now()\n", "    USE_CUDA, DEVICE = get_device()\n", "    parser = argparse.ArgumentParser(description='Baseline for recipe generation (dynamic attn)')\n", "    parser.add_argument('--data-dir', type=str, required=True, help='location of the data corpus')\n", "    parser.add_argument('--vocab-emb-size', type=int, default=50, help='size of word embeddings')\n", "    parser.add_argument('--calorie-emb-size', type=int, default=50, help='size of calorie embeddings')\n", "    parser.add_argument('--ingr-emb-size', type=int, default=10, help='size of ingr embeddings')\n", "    parser.add_argument('--item-emb-size', type=int, default=20, help='size of item embeddings')\n", "    parser.add_argument('--top-k', type=int, default=20, help='top k prior item to attend on')\n", "    parser.add_argument('--nhid', type=int, default=256, help='number of hidden units per layer')\n", "    parser.add_argument('--nlayers', type=int, default=1, help='number of layers')\n", "    parser.add_argument('--batch-size', '-b', type=int, default=24, help='batch size')\n", "    parser.add_argument('--model-path', type=str, required=True,\n", "        help='Path from which to retrieve saved model dict')\n", "    parser.add_argument('--save-dir', type=str, required=True,\n", "        help='Where to save model outputs, graphs, etc.')\n", "    parser.add_argument('--overwrite', '-o', action='store_true', default=False,\n", "        help='Overwrite existing outputs')\n", "    parser.add_argument('--ingr-gru', action='store_true', default=False,\n", "        help='Use BiGRU for ingredient encoding')\n", "    parser.add_argument('--decode-name', action='store_true', default=False,\n", "        help='Multi-task learn to decode name along with recipe')\n", "    parser.add_argument('--ingr-emb', action='store_true', default=False,\n", "        help='Use Ingr embedding in encoder')\n", "    parser.add_argument('--shared-proj', action='store_true', default=False,\n", "        help='Share projection layers for name and steps')\n", "    parser.add_argument('--item-emb', action='store_true', default=False,\n", "        help='Use Ingr embedding in encoder')\n", "    parser.add_argument('--ppx-only', action='store_true', default=False,\n", "        help='Only calculate perplexity (on full test set)')\n", "    parser.add_argument('--n-samples', '-n', type=int, default=1e9, help='sample test items')\n", "    args = parser.parse_args()\n\n", "    # Reproducibility\n", "    seed = 42\n", "    np.random.seed(seed)\n", "    torch.manual_seed(seed)\n", "    torch.cuda.manual_seed(seed)\n\n", "    # Args\n", "    data_dir = args.data_dir\n", "    vocab_emb_dim = args.vocab_emb_size\n", "    calorie_emb_dim = args.calorie_emb_size\n", "    ingr_emb_dim = args.ingr_emb_size\n", "    item_emb_dim = args.item_emb_size\n", "    top_k = args.top_k\n", "    hidden_size = args.nhid\n", "    n_layers = args.nlayers\n", "    model_path = args.model_path\n", "    save_dir = args.save_dir\n", "    overwrite = args.overwrite\n", "    ingr_gru = args.ingr_gru\n", "    ingr_emb = args.ingr_emb\n", "    item_emb = args.item_emb\n", "    decode_name = args.decode_name\n", "    batch_size = args.batch_size\n", "    n_samples = args.n_samples\n", "    shared_proj = args.shared_proj\n", "    ppx_only = args.ppx_only\n", "    '''\n", "    Load data\n", "    '''\n", "    # Get the DFs\n", "    train_df, valid_df, test_df, user_items_df, df_r, ingr_map = load_full_data(data_dir)\n", "    n_items = len(df_r)\n", "    print('{} - Data loaded.'.format(datetime.now() - start))\n\n", "    # Pad recipe information\n", "    N_INGREDIENTS = 0\n", "    if ingr_emb:\n", "        print('INGR EMBEDDING')\n", "        n_ingredients_og = max(chain.from_iterable(df_r['ingredient_ids'].values)) + 1\n", "        PAD_INGR = n_ingredients_og\n", "        N_INGREDIENTS = n_ingredients_og + 1\n", "    df_r = pad_recipe_info(\n", "        df_r, max_name_tokens=MAX_NAME, max_ingredients=MAX_INGR,\n", "        max_ingr_tokens=MAX_INGR_TOK, max_step_tokens=MAX_STEP_TOK\n", "    )\n", "    \n", "    # Number of item\n", "    N_ITEMS = len(df_r)\n", "    # Num Item embedding with pad item\n", "    NUM_ITEM_EMBEDDING = N_ITEMS + 1\n", "    PAD_ITEM_INDEX = N_ITEMS\n", "    tensors_to_load = [\n", "        ('name_tensor', 'name_tokens'),\n", "        ('calorie_level_tensor', 'calorie_level'),\n", "        ('technique_tensor', 'techniques'),\n", "        ('ingr_tensor', 'ingredient_ids' if ingr_emb else 'ingredient_tokens'),\n", "        ('steps_tensor', 'steps_tokens'),\n", "        ('ingr_mask_tensor', 'ingredient_id_mask' if ingr_emb else 'ingredient_mask'),\n", "        ('tech_mask_tensor', 'techniques_mask'),\n", "    ]\n", "    tensor_names, tensor_cols = zip(*tensors_to_load)\n\n", "    # Load tensors into memory\n", "    memory_tensors = load_recipe_tensors(\n", "        df_r, DEVICE, cols=tensor_cols, types=[torch.LongTensor] * len(tensors_to_load)\n", "    )\n", "    memory_tensor_map = dict(zip(tensor_names, memory_tensors))\n", "    print('{} - Tensors loaded in memory.'.format(datetime.now() - start))\n\n", "    # Name padding for item\n", "    memory_tensor_map['name_tensor'] = torch.cat(\n", "        [memory_tensor_map['name_tensor'],\n", "        torch.LongTensor([[PAD_INDEX] * MAX_NAME]).to(DEVICE)]\n", "    )\n", "    if n_samples < len(test_df) and not ppx_only:\n", "        sampled_test = test_df.sample(n=n_samples)\n", "    else:\n", "        sampled_test = test_df\n", "    test_data = DataFrameDataset(sampled_test, ['u', 'i'])\n", "    test_sampler = BatchSampler(test_data, batch_size)\n", "    '''\n", "    Create model\n", "    '''\n", "    model = create_model(\n", "        vocab_emb_dim=vocab_emb_dim, calorie_emb_dim=calorie_emb_dim,\n", "        item_emb_dim=item_emb_dim, n_items_w_pad=NUM_ITEM_EMBEDDING, hidden_size=hidden_size,\n", "        n_layers=n_layers, dropout=0.0, max_ingr=MAX_INGR, max_ingr_tok=MAX_INGR_TOK,\n", "        use_cuda=USE_CUDA, state_dict_path=model_path,\n", "        ingr_gru=ingr_gru, decode_name=decode_name, ingr_emb=ingr_emb,\n", "        num_ingr=N_INGREDIENTS, ingr_emb_dim=ingr_emb_dim, shared_projection=shared_proj,\n", "        item_emb=item_emb\n", "    )\n", "    model_id = os.path.basename(model_path)[:-3]\n", "    model_save_dir = os.path.join(save_dir, model_id)\n", "    if not os.path.exists(model_save_dir):\n", "        os.mkdir(model_save_dir)\n\n", "    # Calculate loss\n", "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n", "    loss_compute = SimpleLossCompute(criterion, None)\n", "    \n", "    # Sample via top-3\n", "    logit_mod = partial(top_k_logits, k=3)\n", "    sample_method = 'multinomial'\n", "    eval_model(\n", "        device=DEVICE,\n", "        model=model,\n", "        sampler=test_sampler,\n", "        loss_compute=loss_compute,\n", "        logit_modifier_fxn=logit_mod,\n", "        token_sampler=sample_method,\n", "        print_every=20,\n", "        max_len=MAX_STEP_TOK,\n", "        user_items_df=user_items_df,\n", "        top_k=top_k,\n", "        pad_item_ix=PAD_ITEM_INDEX,\n", "        max_name_len=MAX_NAME,\n", "        ingr_map=ingr_map,\n", "        pad_ingr=PAD_INGR,\n", "        base_save_dir=model_save_dir,\n", "        ppx_only=ppx_only,\n", "        **memory_tensor_map\n", "    )"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}